\section{Raw data}
The simulations took varying amounts of time to complete with some of the longer
programs taking 30-40 hours per thread version. Therefore, it took around a week
before the majority of the data was available, with only the \texttt{volrend} 
program runs still going; they seemed to take around 70 hours each. Excluding 
the \texttt{volrend} benchmark, there was 320 simulations to retrieve: 8 
benchmarks $\times$ 8 big.LITTLE configurations $\times$ 5 different number of 
threads. A total of 120GB of raw text data was accumulated from the programs 
excluding \texttt{volrend}, with each \texttt{stats.txt} file typically being 
between 2.5-4 million lines long. However, as most of the stats recorded by the 
simulator are much more detailed than what could be recorded and used by using 
PMUs, the extracted data size is much smaller.

    \subsection{Problems encountered}
        \subsubsection{Network outage}
        As mentioned, the \texttt{volrend} benchmark took significantly longer 
        than the other programs being run on the simulator. Unfortunately, this 
        meant that it was still running during the University-wide network 
        outage that occurred on the 9\textsuperscript{th} of May. Although the 
        vast majority of the University's network infrastructure was restored 
        quickly, it took a while before access to the cluster was 
        re-established. Additionally, it seems the network outage affected the 
        \texttt{tmux} sessions that were being used to run the benchmarks, 
        despite them being local to each micro server. As a result the 
        \texttt{volrend} data was incomplete and was inaccessible until shortly 
        before the deadline. Because of these issues, the \texttt{volrend} data 
        was not used for the project.
    
        \subsubsection{Benchmark crashes}
        Each bootscript was set to print the line ``\texttt{Benchmark done, 
        exiting simulation.}'' once the benchmark had returned. This allowed me 
        to easily be able to tell if the runs had completed successfully, in 
        case some of them crashed. Unfortunately, once the data was downloaded 
        from the various micro servers and assembled in one place, it became 
        apparent that despite running for a long time, a lot of the benchmarks 
        seemed to have crashed near the end of their execution. By counting the 
        number of times the ``\texttt{Benchmark done}[...]'' line occurred, the 
        following completion ratios were obtained:
        \begin{table}[H]
            \centering
            \begin{tabular}{l|c|c|r}
                \textbf{benchmark} & \textbf{n\_finished} & \textbf{n\_crashed} 
                & \textbf{completion\_ratio} \\
                \hline
                \texttt{barnes} & 20 & 20 & 50.0\% \\
                \texttt{fmm} & 9 & 31 & 22.5\% \\
                \texttt{ocean-contiguous\_partitions} & 13 & 27 & 32.5\% \\
                \texttt{ocean-non\_contiguous\_partitions} & 0 & 40 & 0.0\% \\
                \texttt{radiosity} & 13 & 27 & 32.5\% \\
                \texttt{raytrace} & 8 & 32 & 20.0\% \\
                \texttt{water-nsquared} & 9 & 31 & 22.5\% \\
                \texttt{water-spatial} & 5 & 35 & 12.5\% \\
            \end{tabular}
            \caption{Completion ratios for the various benchmarks}
        \end{table}
        As mentioned earlier, most of the crashed benchmarks seem to have 
        crashed towards the end of their execution. The exception is the 
        \texttt{ocean-non\_contiguous\_partitions}, which seems to have crashed 
        within milliseconds of starting. When plotting the power usage of the 
        cores for some of the benchmarks (Appendix \ref{ch:power-plots}), it 
        becomes clear that the crashes occurred very near the end of the 
        benchmark runs. If one were to look purely at the plots, it would be 
        difficult to say which crashed and which did not based on their power 
        consumption and time taken. Therefore, the data from the crashed 
        benchmarks was kept in the final data set.
    
\section{Data extraction}
    \subsection{Mapping gem5 stats to PMU events}
    In order to keep the data as realistic as possible so as to potentially be 
    able to implement the results on real hardware, only data entries which 
    could be obtained through the PMUs were kept. Not all data that the PMUs 
    are capable of recording is available in the gem5 stats. For example, while 
    there is a detailed overview of what amount of data was sent over the 
    memory bus, how many packets were used, and which parts sent them, there 
    does not seem to be an equivalent of the PMU event \texttt{0x19}: ``Bus 
    access''. Some stats also require slightly greater care to extract, e.g. 
    the number of architecturally executed/committed instructions (PMU event 
    \texttt{0x08}) is recorded as \textsf{[...]commit.committedInsts}'' for 
    CPUs in the big cluster but simply as ``\textsf{[...].committedInsts}'' for 
    CPUs in the LITTLE cluster. Since the stats blocks in the files are often 
    1000s of lines, these subtleties are easy to miss when skimming through the 
    stats files. Fortunately, most of the PMU events have a clear, direct 
    equivalent in the stats file. The complete map between stats and PMU events 
    is:
    \begin{table}[H]
        \centering
        \begin{tabular}{r|c|l}
            \textbf{gem5 stat} & \textbf{PMU event} & \textbf{description} \\
            \hline
            \textsf{branchPred.BTBLookups} & \texttt{0x12} & Predictable branch
                speculatively executed \\ 
            \textsf{committedInsts} & \texttt{0x08} & Instruction 
                architecturally executed \\
            \textsf{branchPred.condIncorrect} & \texttt{0x10} & 
            \begin{tabular}[c]{@{}l@{}}Mispredicted or
                not predicted branch\\ speculatively executed\end{tabular} \\
            \textsf{numCycles} & \texttt{0x11} & Cycle {[count]} \\
            \textsf{icache.overall\_accesses::total} & \texttt{0x14} & L1
                instruction cache access \\
            \textsf{dcache.overall\_accesses::total} & \texttt{0x04} & L1 data
                cache access \\
            \textsf{dcache.writebacks::total} & \texttt{0x15} & L1 data cache
                Write-Back \\
            \textsf{l2.overall\_accesses::total} & \texttt{0x16} & L2 data cache
                access \\
            \textsf{l2.writebacks::total} & \texttt{0x18} & L2 data cache
                Write-Back \\
        \end{tabular}
        \caption{Mapping between gem5 stats and PMU events (in order of 
        occurrence in the stats files)}
    \end{table}

    The branch prediction stats were somewhat challenging to derive. Initially,
    I believed that the ``\textsf{branchPred.condPredicted}'' corresponded to 
    the PMU event for speculatively executing a branch. However, looking 
    through the source code, I discovered that the stat is always incremented 
    \textit{before} predicting if the branch is taken or not (Figure 
    \ref{subfig:bp-condpredicted-wrong}). Additionally, I discovered that the 
    PMU registered to the gem5 Simulator seems to get incremented each time the 
    branch predictor is used instead of each time a ``[p]redictable branch [is] 
    speculatively executed'' (Figure \ref{subfig:bp-lookups-wrong}). The 
    \textsf{BTBLookups} stat is the only viable option, as it is incremented 
    each time a conditional branch (not a return address) is predicted as taken 
    (Figure \ref{subfig:bp-btblookups-correct}).
    \begin{figure}[H]
        \centering
        \begin{subfigure}{0.55\linewidth}
            \centering
            \includegraphics[width=\textwidth]{screenshots/br-pred/br-pred-pmu-0x12-condPredicted-src.png}
            \caption{\texttt{condPredicted} increments before the prediction}
            \label{subfig:bp-condpredicted-wrong}
        \end{subfigure}
        \begin{subfigure}{0.55\linewidth}
            \centering
            \includegraphics[width=\linewidth]{screenshots/br-pred/br-pred-pmu-0x12-maybe.png}
            \caption{\texttt{lookups}, which is wired up to the gem5 PMU, seems
                     to get incremented on each branch predictor access}
            \label{subfig:bp-lookups-wrong}
        \end{subfigure}
        \begin{subfigure}{0.55\linewidth}
            \centering
            \includegraphics[width=\linewidth]{screenshots/br-pred/br-pred-pmu-0x12-if-pred-taken.png}
            \caption{\texttt{BTBLookups} is incremented if the branch was a
                     conditional branch \emph{and} it was predicted as taken}
            \label{subfig:bp-btblookups-correct}
        \end{subfigure}
        \caption{The source C++ code of various branch predictor stats}
    \end{figure}
    For branch mispredictions, the \textsf{condIncorrect} fortunately seems to 
    be correct and (according to the code comments) only get incremented when 
    the branch was discovered to be mispredicted.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.55\linewidth]{screenshots/br-pred/br-pred-pmu-0x10-condIncorrect-src.png}
        \caption{The C++ code confirms that \texttt{condIncorrect} stat 
                 monitors branch mispredictions}
    \end{figure}

    \subsection{The \texttt{data-aggregate.py} script}
    Having derived which PMU events could be extracted from the gem5 stats, I 
    wrote a script which would scrape each of the many stat files, extract, and 
    aggregate the stats into a single file. The script takes the top-level 
    directory which contains the benchmark output directories as structured 
    through the experimental setup. Since the path to each stats file contains 
    information as to what benchmark, which setup, and how many threads were 
    used, this information is collected when going down the directory 
    structures. For each stat block in the stats file, a line in a CSV-file is 
    then created. Each line contains the information stored in path/directory 
    names, a time increment in the simulation, all the PMU stats, and the power 
    stats. It is possible to specify the name of the output file through the 
    \texttt{--output} flag.
    
    Running the script on the 120GB of collected data produced a CSV file of 
    101MB. This is still a lot of text, but it is also much more manageable than
    the 120GB of full-detail data.

\section{Data Processing}
    \subsection{Pre-processing}
    The extracted data needed to be pre-processed before it could be used. For 
    the comparisons between PMU measurements to be fair, the measurements are 
    divided by the number of cycles. Each time slice of 1ms can contain a 
    different number of cycles simulated, due to varying number of stalls or 
    less activity on the core(s), so by dividing the PMU measurements by the 
    number of cycles simulated that time slice, the comparisons become fairer 
    than comparing the time slices directly. The static and dynamic power 
    measurements were added up to be one `\texttt{total\_power}' measurement.
    
    By creating a multi-index dataframe ordered by benchmark name, big.LITTLE 
    configuration, number of threads, time slice, cluster, and CPU, the data 
    can then be added up over time, resulting in one entry per benchmark, 
    configuration, number of threads, cluster, and CPU which can be easily 
    compared to other instances and/or other benchmarks in order to determine 
    which setup(s) were best in terms of power savings. To be able to compare 
    the total power of the different runs, the power was normalised using 
    Min-Max Feature scaling. This fits the measurements into a range between 0 
    and 1 inclusive, based on the minimum and maximum values in the dataset, 
    which then makes the power measurements easier to plot and compare since 
    all the records will at most have a power value of 1, whilst also 
    maintaining the relative size differences. The scaling was done using the 
    \texttt{minmax\_scale} function from Scikit-Learn's 
    \cite{pedregosa_scikit-learn_2011} \texttt{preprocessing} module.
    
    \subsection{Predicting and Plotting}
    Since it was too late to try to implement a new scheduler, we instead 
    decided to look at whether the recorded data could show that a more 
    power-efficient schedule existed. The idea was as follows:
    \begin{enumerate}
        \item Select a benchmark that will be arriving as a new program.
        \item Get the PMU data by `running' it on a stock big.LITTLE
              configuration. This should reveal some details about the program
              and can simply be looked up since we have all the runs already.
        \item Based on the PMU data, predict what benchmark is the most similar
              by looking at the data which does not contain any information 
              about the benchmark run, for some definition of `similar'.
        \item Find the optimal big.LITTLE configuration for that benchmark, for
              some definition of `optimal'.
        \item `Run' the actual benchmark on the configuration and measure/look
              up the actual results.
        \item Based on the data containing all entries, find the true optimal
              configuration for the benchmark, along with its results.
        \item Compare the predicted and actual results, along with a baseline.
    \end{enumerate}

    To keep the predictions and comparisons simple, only data points from 
    configurations where the number of threads of the benchmark was equal to 
    the number of cores available were kept. The rationale was that this 
    optimised the usage of the configuration and so allowed us to see the 
    fastest the benchmark could perform on that setup, allowing us to see if and
    how the benchmark scaled with the number of big and LITTLE cores available.
    
    We decided to use Nearest-Neighbours with Euclidean Distance to determine 
    similarity. This was done using the default settings for the 
    \texttt{KNearestNeighborsClassifier} in Scikit-Learn's 
    \cite{pedregosa_scikit-learn_2011} \texttt{neighbors} module. The stock 
    configuration was set to be a 2 big 2 LITTLE configuration because it was a 
    configuration which seemed to be somewhere in the middle, hopefully 
    allowing for balanced predictions in either direction. For the same reasons,
    this configuration was also used as the baseline.
    
    The predictions were done, leaving each benchmark out in turn and 
    retraining the module each time. Since the model returned multiple 
    predictions, one for each core in the stock configuration, these were 
    treated as votes and the benchmark with the most votes was selected for 
    comparison. Based on which benchmark was predicted to be most similar, the 
    optimal configuration for that benchmark was then found by minimising all 
    the number of cycles and amounts of power consumed by the setup for the most
    most similar benchmark. To compare across different configurations, which 
    have a different number of cores to compare, the power was accumulated and 
    only the highest number of cycles per cluster was kept. This was done since 
    power consumption is additive, and the cores and clusters were run
    concurrently, meaning the greatest number of cycles will indicate the last 
    point at which the final thread of the benchmark finished. It was possible 
    to get more than one configuration back if they were equally optimal, 
    however no such situation occurred
    
    Taking the most similar benchmark and its optimal configuration, the data 
    from the corresponding run with the number of threads equal to the total 
    number of cores was retrieved, as was the data from the actual optimum and
    the baseline data.
    
    For each benchmark, a plot showing the number of cycles and amount of power 
    for the big and LITTLE clusters was created. These plots allowed me to 
    easily spot and compare the performance of the various benchmarks and 
    configurations. Additionally, in order to get a different look at how the 
    predicted optimum compared to the actual one, these were plotted against 
    each other.
    
\section{Analysis}
    \subsection{Plots and observations}
    The behaviour of the Nearest-Neighbours model for the different benchmarks 
    is largely identical. It seems to almost always result in the optimal 
    configuration being 4b4L, apart from when predicting for the \texttt{barnes}
    benchmark, where the 1b1L configuration is predicted to be the most optimal.
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{result-plots/stock-2b2L/clusters-bars.png}
        \caption{Per-cluster comparisons using 2b2L as the baseline}
        \label{fig:clusters-bars}
    \end{figure}
    
    Looking at the plots (note the y-axis for cycles is in Giga, i.e. 
    \textsf{1e9}, cycles), it becomes apparent that the trade-offs are there, 
    and that the model has been mostly successful in picking the optimal 
    configuration. For the \texttt{barnes} benchmark, the power consumption of 
    the predicted optimal configuration is not far off the true optimum. 
    However, the largest number of cycles is. The \texttt{fmm} benchmark reveals
    an interesting caveat of the optimum finding algorithm: the performance of
    the true optimum is worse than the predicted, but the power is not. In 
    fact, when looking at the system as a whole, the total power consumption of 
    the 1b1L setup is less than the 4b4L setup. What has likely happened is 
    that when minimising the number of cycles and the power used, the 1b1L 
    configuration was the first to be tried. Subsequently, although the 4b4L 
    configuration has better performance, it does not \emph{also} have better 
    power performance and so it was not selected. It is possible this could be 
    fixed by making the optimum finding algorithm consider the scale of the 
    trade-offs, and not just the values.
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{result-plots/stock-2b2L/system-bars.png}
        \caption{System-wide comparisons using 2b2L as the baseline}
        \label{fig:system-bars}
    \end{figure}

    Interestingly, the tactic of always going with 4b4L does not seem bad when 
    looking at the overall performance. Each time a different configuration 
    manages to do better on one cluster, it seems to come at a cost on the 
    other cluster. This is likely due to 4b4L requiring more power in theory, 
    but finishing the benchmarks so much faster that it does not matter.
    
    \subsection{Evaluating the prediction results}
    Plotting the true optima against the predicted optima, helps show how close 
    most of predictions are. The majority of the benchmarks lie exactly on the 
    $y=x$ line, indicating that the prediction was entirely correct. The plots 
    also help explain how the prediction and true optima were off for the 
    \texttt{barnes} and \texttt{fmm} benchmarks respectively. The power 
    measurements are clearly very close to agreeing, but the performance 
    measurements are much further away from the $y=x$ line. This suggests that, 
    as mentioned with the optimum finding algorithm, some balancing to consider 
    the scale of improvements would be beneficial. That being said, the 
    geometric mean, taken across all the benchmarks and plotted as a cyan dot, 
    lying very close to the $y=x$ line is a good indication that the model was 
    good at predicting the right setup overall.
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{result-plots/stock-2b2L/system-scatter.png}
        \caption{Plotting the predicted ideal against the true ideal, for
                 system-wide maximum number of cycles and total power use}
    \end{figure}

    Although the sample size is small, computing the improvement ratios of the 
    maximum number of cycles and the total power used by dividing the ideal 
    values by the predicted ones, and then calculating the geometric mean of 
    these ratios seems highly promising. The geometric mean comes out to be 
    $\approx 98.77\%$ for the cycle ratios and $\approx 99.28\%$ for the power
    ratios. This confirms that there is little to be improved in terms of the
    power predictions, and also seems to suggest that despite the size of the 
    performance outliers, the space for improvement may be smaller than 
    initially anticipated. However, as mentioned earlier, this is based on a 
    small sample size and more data would be required to say for certain.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{screenshots/promising-geomeans.png}
        \caption{The geometric means for improvement ratios seem very promising}
    \end{figure}
    
    \subsection{Using other stock configurations}
    In order to improve performance of the model, more data could be supplied. 
    One way of doing this would be to `run' the arriving benchmark on more than 
    one stock configuration and using the results from all the stock 
    configurations as input when predicting the most similar. I tried using 
    pairs of configurations: 2b2L+4b4L, 1b1L+2b2L. The idea was that the change 
    in balance between the configurations would lead to a change in predictions 
    as there were `examples' of which cores affected what. However, when 
    plotted, the results were identical. The resulting system-wide plots can be 
    seen in Appendix \ref{ch:multis-pred-res}. The complete set of plots can be 
    found in the \texttt{results-plots} directory.
