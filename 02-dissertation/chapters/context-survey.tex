\section{The CFS and multicore systems}
It is only fairly recently that people have started looking at the problem of
Energy Aware Scheduling (EAS) and optimising for big.LITTLE  In 2016 the CFS
received several fixes based on a paper published in the same year, detailing
how some bugs in the CFS decreased performance on multicore systems by 13-24\%
\cite{lozi_linux_2016}. This paper addresses multicore in general, and does not
focus on big.LITTLE or energy. However, it did drastically improve the
performance of the CFS on multicore systems. As mentioned in Jibaja et al.
\cite{jibaja_portable_2016}, the CFS does well on big.LITTLE for scalable
workloads, but it may be possible to accelerate other, non-scalable workloads as
well.

\section{Adding big.LITTLE awareness to a scheduler}
    \subsection{The WASH scheduler}
    Jibaja et al. describe the ``WASH'' scheduler \cite{jibaja_portable_2016}. 
    The WASH scheduling algorithm is an attempt at optimising big.LITTLE setups 
    on the fly rather than through external hints or guidance from the 
    programmer. By analysing the performance of various tasks depending on the 
    percentage of cycles spent waiting on locks, WASH estimates the 
    criticality, sensitivity, and progress of the tasks currently active on the 
    system. Since the number of instructions executed varies between big and 
    LITTLE cores (due to their different computational power), WASH scales with 
    respect to the instructions per clock (IPC) so as to have a more fair view 
    of a task's performance. Based on all this information, WASH wash then 
    `accelerates' tasks by either assigning them to a big core from the 
    beginning, or migrating them to a big core if not enough progress is being 
    made according to the parameters monitored. This is done through the 
    POSIX-thread (pthread) API, specifically the 
    \texttt{pthread\_setaffinity\_np} function which sets the affinity of the 
    pthread to the CPU(s) given. Whilst this is a straightforward way to migrate
    threads, it also means the actual migration is controlled by the CFS rather
    than WASH itself \cite{yu_colab_2020}. WASH's main focus is to make the
    scheduler big.LITTLE aware and thereby hopefully improve power usage, and as
    such it does not focus on power or energy along with big.LITTLE, meaning 
    there is likely to be room for improvement. Finally, it was discovered by 
    Yu et al. \cite{yu_colab_2020} that WASH's decision-making does not always 
    accelerate the program as a whole, despite running computationally heavier 
    tasks on big cores. Counterintuitively, heavy tasks running on big cores can
    sometimes be slowed down, due to waiting on results or feedback from lighter
    tasks running on LITTLE cores, and so it would be better to accelerate these
    lighter tasks.
    
    \subsection{The COLAB scheduler}
    The paper describing the COLAB scheduler, published by Yu et al. earlier 
    this year \cite{yu_colab_2020}, highlights a number of shortcomings 
    regarding the WASH scheduler and proposes a different, collaborative 
    scheduling algorithm. The COLAB scheduler addresses \textit{thread 
    criticality} (some threads being more critical with respect to program 
    performance than others) and \textit{core sensitivity} (the big and LITTLE 
    cores being designed for different types of workload) whilst maintaining 
    fairness. The collaborative part of COLAB comes from allowing both the core 
    allocator (deciding which core to start on) and the thread selector to 
    label the threads in terms of whether they have a good chance of a high 
    speedup from a big core and whether they are highly blocking or not. The 
    speedup label can then be used to allocate the best type of core for the 
    thread, and a more blocking thread can be selected more often so as to 
    hopefully move it, and by extension threads waiting on it, along faster. 
    This collaboration between core allocation and thread selection allows 
    COLAB to achieve performance gains of 5-15\% on average compared to WASH, 
    depending on the hardware configuration. However, similar to WASH, COLAB 
    does not address power or energy, but focuses on improving big.LITTLE. 
    Therefore, this space is still mostly unexplored and it is possible power 
    savings could be made without sacrificing much performance.

\section{Runtime DVFS management via PMUs}
A recent Ph.D. thesis and collection of papers by K. Reddy Basireddy do tackle
the DVFS side of things. The thesis seems address power and energy for both
asymmetric and symmetric multicore processors, with DVFS being a subset of the
methods examined \cite{basireddy_runtime_2019}. The DVFS part resulted in the
AdaMD paper \cite{basireddy_adamd_2019} which details how by using certain PMUs
with a ``performance monitor'', the DVFS settings can be adjusted to balance
power and performance at runtime. AdaMD works by doing an initial prediction of
what it thinks the best DVFS and core settings are, and then periodically
monitoring the performance of the application and adjusting the DVFS settings
and core allocation in terms of ``performance constraints''. However, the paper
seems somewhat vague in terms of where these performance constraints come from
and what they are defined in terms of, e.g. if they are the number of cycles or
application runtime or something different. Additionally, as its main focus is
DVFS and energy efficiency, and since the paper was published before the COLAB
paper, it is possible that the performance of COLAB and the energy efficiency
could be combined.

\section{ARM's EAS in the Linux kernel}
ARM have also looked into EAS. With release 5.0 of the Linux kernel in March
2019 \cite{torvalds_linux_2019} EAS was officially merged into the kernel and
released \cite{noauthor_linux_2019}. However, as described in the official
documentation \cite{noauthor_energy_nodate} this relies on the Energy Model
framework being present in the kernel, rather than the PMUs, and also appears 
to still be a work in progress. The Energy Model framework is likely to be more 
expensive to use than the hardware-based PMUs, and it is possible that greater 
performance monitoring accuracy could be achieved by using certain PMU events. 
Finally, similar to AdaMD, EAS on Linux does not seem to account for the 
improvements described in the COLAB paper and so it is possible that greater 
performance and energy savings could be achieved.

\section{Summary}
To summarise, making the scheduler aware of the different core types on
Asymmetric Multicore Processors is something only recently started gaining
attention in the literature. Making the scheduler energy aware is also only
recently beginning to make its way into real systems, e.g. EAS in Linux 5.x.
Across the existing literature, nothing seems to combine the two, resulting in a
highly interesting and applicable research area, as big.LITTLE systems become
increasingly ubiquitous in both mobile devices and the IoT, both of which are
could benefit from power savings and/or better scheduling.
